version: 1.2.8

cache: true

mcpServers:
  time:
    type: stdio
    chatMenu: true
    command: uvx
    args:
      - mcp-server-time
      - --local-timezone=America/Santiago
  search:
    type: stdio
    chatMenu: true
    command: npx
    args:
      - -y
      - tavily-mcp@latest
    env:
      TAVILY_API_KEY: "${TAVILY_API_KEY}"

endpoints:
  agents:
    capabilities:
      - "file_search"
      - "actions"
      - "tools"
      - "artifacts"
      - "chain"
  custom:
    # groq
    # Model list: https://console.groq.com/settings/limits
    - name: "groq"
      apiKey: "${GROQ_API_KEY}"
      baseURL: "https://api.groq.com/openai/v1/"
      models:
        default:
          - allam-2-7b
          - compound-beta
          - compound-beta-mini
          - deepseek-r1-distill-llama-70b
          - gemma2-9b-it
          - llama-3.1-8b-instant
          - llama-3.3-70b-versatile
          - llama-guard-3-8b
          - llama3-70b-8192
          - llama3-8b-8192
          - meta-llama/llama-4-maverick-17b-128e-instruct
          - meta-llama/llama-4-scout-17b-16e-instruct
          - meta-llama/llama-guard-4-12b
          - mistral-saba-24b
          - playai-tts
          - playai-tts-arabic
          - qwen-qwq-32b
        fetch: false
      titleConvo: true
      titleModel: "mixtral-8x7b-32768"
      modelDisplayLabel: "groq"

memory:
  disabled: false
  personalize: true
  tokenLimit: 8000
  messageWindowSize: 10
  agent:
    provider: "openAI"
    model: "gpt-4.1-mini"
    model_parameters:
      temperature: 0.3

interface:
  runCode: false
  webSearch: false
  customWelcome: "Hello {{user.name}}"
